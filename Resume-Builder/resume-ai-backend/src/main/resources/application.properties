spring.application.name=resume-ai-backend


spring.ai.ollama.chat.model=deepseek-r1:1.5b
spring.ai.ollama.chat.options.temperature=0.7
# Increase timeout for local AI generation (default is often too short)
spring.ai.ollama.connection-timeout=60s
spring.ai.ollama.read-timeout=900s